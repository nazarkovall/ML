{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import or define evaluate_classification, plot_decision_boundary, plot_data\n",
    "\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename, data_columns, target_column):\n",
    "    \"\"\"Load dataset from CSV file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to CSV file.\n",
    "        data_columns (list): List of column names for data.\n",
    "        target_column (str): Name of target column.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing data and target.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(filename)\n",
    "    X, y = df[data_columns], df[target_column]\n",
    "    return X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"Logistic Regression Classifier.\"\"\"\n",
    "    def __init__(self, standardize=True, \n",
    "                 learning_rate=0.01, \n",
    "                 max_iter=1000,\n",
    "                 tol=1e-4,\n",
    "                 verbose=False):\n",
    "        \"\"\"Initialize Logistic Regression Classifier.\n",
    "        \n",
    "        Args:\n",
    "            standardize (bool): Whether to standardize the data.\n",
    "            learning_rate (float): Learning rate for gradient descent.\n",
    "            max_iter (int): Maximum number of iterations for gradient descent.\n",
    "            tol (float): Tolerance for gradient descent.\n",
    "            verbose (bool): Whether to print cost at each 100th iteration.\n",
    "        \"\"\"\n",
    "        self.standardize = standardize\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def normalize(self, X):\n",
    "        \"\"\"Normalize the data.\n",
    "        \n",
    "        Args:\n",
    "            X (array): Data to normalize.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Tuple containing normalized data, mean, and standard deviation.\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        mean = np.mean(X, axis=0)\n",
    "        std = np.std(X, axis=0)\n",
    "        X_new = (X-mean)/std\n",
    "        return X_new, mean, std\n",
    "\n",
    "    def add_intercept(self, X):\n",
    "        \"\"\"Add intercept term to the data.\n",
    "        \n",
    "        Args:\n",
    "            X (array): Data to add intercept term.\n",
    "        \n",
    "        Returns:\n",
    "            array: Data with intercept term.\"\"\"\n",
    "        m = X.shape[0]\n",
    "        ones = np.ones((m, 1))\n",
    "        X_new = np.column_stack((ones, X))\n",
    "        return X_new\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid function.\n",
    "        \n",
    "        Args:\n",
    "            z (array): Input to sigmoid function.\n",
    "            \n",
    "        Returns:\n",
    "            array: Output of sigmoid function.\"\"\"\n",
    "        # TODO: Implement\n",
    "        h = 1/(1 + np.exp(-z))\n",
    "        return h\n",
    "    \n",
    "    def hypothesis(self, X, theta):\n",
    "        \"\"\"Hypothesis function.\n",
    "        \n",
    "        Args:\n",
    "            X (array): Data.\n",
    "            theta (array): Parameters.\n",
    "        \n",
    "        Returns:\n",
    "            array: Output of hypothesis function.\"\"\"\n",
    "        # TODO: Implement\n",
    "        z = np.dot(X,theta)\n",
    "        return self.sigmoid(z)\n",
    "\n",
    "    def cost_function(self, X, y, theta):\n",
    "        \"\"\"Cost function.\n",
    "        \n",
    "        Args:\n",
    "            X (array): Data.\n",
    "            y (array): Target.\n",
    "            theta (array): Parameters.\n",
    "            \n",
    "        Returns:\n",
    "            float: Cost of hypothesis function.\"\"\"\n",
    "        # TODO: Implement\n",
    "        m = X.shape[0]\n",
    "        if m == 0:\n",
    "            return None\n",
    "        #transpose error????/\n",
    "        A = self.hypothesis(X,theta)\n",
    "        cost = -(np.dot(y.T, np.log(A)) + np.dot((1-y).T,np.log(1-A)))/m\n",
    "        return cost\n",
    "\n",
    "    def gradient(self, X, y, theta):\n",
    "        \"\"\"Gradient of cost function.\n",
    "        \n",
    "        Args:\n",
    "            X (array): Data.\n",
    "            y (array): Target.\n",
    "            theta (array): Parameters.\n",
    "            \n",
    "        Returns:\n",
    "            array: Gradient of cost function.\"\"\"\n",
    "        # TODO: Implement\n",
    "        A = self.hypothesis(X,theta)\n",
    "        grad = np.dot(X.T, (A-y))\n",
    "        return grad\n",
    "\n",
    "    def gradient_descent(self, X, y, theta):\n",
    "        \"\"\"Gradient descent algorithm.\n",
    "\n",
    "        Args:\n",
    "            X (array): Data.\n",
    "            y (array): Target.\n",
    "            theta (array): Parameters.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Tuple containing parameters and costs.\"\"\"\n",
    "        costs = []\n",
    "        J = self.cost_function(X, y, theta)\n",
    "        costs.append(J)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Iteration 0 Cost: {J}\")\n",
    "\n",
    "        for i in range(1, self.max_iter + 1):\n",
    "            # TODO: Implement\n",
    "            grad = self.gradient(X,y,theta)\n",
    "            theta = theta - self.learning_rate*grad\n",
    "            cost = self.cost_function(X,y,theta)\n",
    "            \n",
    "            costs.append(cost)\n",
    "\n",
    "            if i % 100 == 0 and self.verbose:\n",
    "                print(f\"Iteration {i} Cost: {cost}\")\n",
    "\n",
    "            if np.abs(costs[i] - costs[i - 1]) < self.tol:\n",
    "                print(f\"Converged at iteration {i}\")\n",
    "                break\n",
    "\n",
    "        return theta, costs\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model.\n",
    "\n",
    "        Args:\n",
    "            X (array): Data.\n",
    "            y (array): Target.\"\"\"\n",
    "        if self.standardize:\n",
    "            X_new, self.mean, self.std = self.normalize(X)\n",
    "        X_new = self.add_intercept(X_new)\n",
    "\n",
    "        self.theta = np.zeros(X_new.shape[1])\n",
    "        self.theta, self.costs = self.gradient_descent(X_new, y, self.theta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the target.\n",
    "\n",
    "        Args:\n",
    "            X (array): Data.\n",
    "\n",
    "        Returns:\n",
    "            array: Predicted target.\"\"\"\n",
    "        # TODO: Implement\n",
    "        if self.standardize:\n",
    "            X_new,mean,std = self.normalize(X)\n",
    "        X_new = self.add_intercept(X_new)\n",
    "        \n",
    "        \n",
    "        proba_vec = self.hypothesis(X_new, self.theta)\n",
    "        y_pred = [1 if i > 0.5 else 0 for i in proba_vec]\n",
    "        return y_pred\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict the probability of target.\n",
    "        \n",
    "        Args:\n",
    "            X (array): Data.\n",
    "        \n",
    "        Returns:\n",
    "            array: Predicted probability of target.\"\"\"\n",
    "        # TODO: Implement\n",
    "        if self.standardize:\n",
    "            X_new,mean,std = self.normalize(X)\n",
    "        X_new = self.add_intercept(X_new)\n",
    "\n",
    "        h = self.hypothesis(X_new, self.theta)\n",
    "        return np.column_stack((1-h, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(y_true, y_pred, labels = None):\n",
    "    \"\"\"\n",
    "    Evaluate classification model performance.\n",
    "    Args:        y_true (array): Ground truth values.\n",
    "    y_pred (array): Predicted values.\n",
    "    labels (list): List of labels to index the matrix.\n",
    "    Returns:        accuracy (float): Accuracy score.    \n",
    "    report (pd.DataFrame): Classification report.\n",
    "    confusion_matrix (pd.DataFrame): Confusion matrix.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    confusion = pd.DataFrame(confusion_matrix(y_true, y_pred), labels, labels)\n",
    "    return accuracy, report, confusion\n",
    "\n",
    "def plot_data(X, y):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "\n",
    "        \n",
    "def plot_decision_boundary(model, X, y):\n",
    "    x_span = np.linspace(min(X[:, 0]) - 0.25, max(X[:, 0]) + 0.25, 50)\n",
    "    y_span = np.linspace(min(X[:, 1]) - 0.25, max(X[:, 1]) + 0.25, 50)\n",
    "    xx, yy = np.meshgrid(x_span, y_span)\n",
    "    xx_, yy_ = xx.ravel(), yy.ravel()\n",
    "    grid = np.c_[xx_, yy_]\n",
    "    pred_func = model.predict_proba(grid)[:,0]\n",
    "    z = pred_func.reshape(xx.shape)\n",
    "    c = plt.contourf(xx, yy, z, cmap=\"RdYlGn\")\n",
    "    plt.colorbar(c)\n",
    "    colors = list(mcolors.TABLEAU_COLORS.keys())\n",
    "    color_values = [colors[int(label)] for label in y]\n",
    "    plt.scatter(X[:, 0], X[:, 1], marker=\"x\", c=color_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples = 200, n_classes = 2, n_features = 2, \n",
    "                           n_informative=2, n_redundant=0, random_state = 42,\n",
    "                           flip_y=0.02, class_sep=0.8)\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(standardize=True, learning_rate=0.01, max_iter=1000, tol=1e-4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, report, confusion = evaluate_classification(y_test, y_test_pred)\n",
    "print(f\"Accuracy: \\n{accuracy}\")\n",
    "print(f\"Report: \\n{report}\")\n",
    "print(f\"Confusion: \\n{confusion}\")\n",
    "plot_decision_boundary(lr, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "lr = LogisticRegression(standardize=True, learning_rate=0.01, max_iter=1000, tol=1e-4, verbose=True)\n",
    "lr.fit(X_train, y_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "\n",
    "accuracy, report, confusion = evaluate_classification(y_test, y_test_pred)\n",
    "print(f\"Accuracy: \\n{accuracy}\")\n",
    "print(f\"Report: \\n{report}\")\n",
    "print(f\"Confusion: \\n{confusion}\")\n",
    "plot_decision_boundary(lr, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the above LogisticRegression class on datasets sats.csv and tests.csv. Consider using polynomial features when applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = [\"exam1\", \"exam2\"]\n",
    "target_column = \"submitted\"\n",
    "X, y = load_datasets('../../Data/Classification/sats.csv', data_columns, target_column)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(standardize=True, learning_rate=0.01, max_iter=1000, tol=1e-4, verbose=True)\n",
    "lr.fit(X_train, y_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "\n",
    "accuracy, report, confusion = evaluate_classification(y_test, y_test_pred)\n",
    "print(f\"Accuracy: \\n{accuracy}\")\n",
    "print(f\"Report: \\n{report}\")\n",
    "print(f\"Confusion: \\n{confusion}\")\n",
    "plot_decision_boundary(lr, X_test, y_test)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "8df173b65e7d4911fe3a36523b49f8a295b4f9ef02cf106a3159aa92aa441c3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
